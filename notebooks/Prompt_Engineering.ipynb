{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tvDx6Gq2x-vB"
      },
      "outputs": [],
      "source": [
        "# !pip install --quiet openai\n",
        "# !pip install --quiet python-dotenv"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hd-eTOx8zExU"
      },
      "source": [
        "# On local machine"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "gather": {
          "logged": 1725995701245
        },
        "id": "3AWzM5Wkx5kE",
        "outputId": "b0909634-16c0-465d-964c-b55d7dfd06c9"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "\n",
        "from openai import AzureOpenAI\n",
        "\n",
        "from dotenv import load_dotenv\n",
        "load_dotenv()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MZZG0oBAMsLC"
      },
      "outputs": [],
      "source": [
        "client = AzureOpenAI(\n",
        "    azure_endpoint = os.getenv('AZURE_ENDPOINT'),\n",
        "    api_key = os.getenv('API_KEY'),\n",
        "    api_version = os.getenv('API_VERSION')\n",
        ")\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Niqzgd3rzQLB"
      },
      "outputs": [],
      "source": [
        "from google.colab import userdata\n",
        "userdata.get('secretName')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pdXsYGy7zL4d"
      },
      "source": [
        "# On Google Colab"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6nqfK2wm5yNv"
      },
      "outputs": [],
      "source": [
        "#On left in secret, add values of the below secrets to access them:\n",
        "import os\n",
        "\n",
        "from openai import AzureOpenAI\n",
        "from google.colab import userdata\n",
        "\n",
        "client = AzureOpenAI(\n",
        "    azure_endpoint = userdata.get('AZURE_ENDPOINT'),\n",
        "    api_key = userdata.get('API_KEY'),\n",
        "    api_version = userdata.get('API_VERSION')\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "gather": {
          "logged": 1725995733597
        },
        "id": "fpuGU1h-x5kF",
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "outputId": "11ef5856-9db9-4e28-b6a9-cdbdaa736df6"
      },
      "outputs": [],
      "source": [
        "prompt = \"What is the capital of France\"\n",
        "\n",
        "response = client.chat.completions.create(\n",
        "    model = \"gpt-4o-mini\",\n",
        "    messages=[\n",
        "        {\"role\": \"system\", \"content\": \"You are a helpful assistant\"},\n",
        "        {\"role\": \"user\", \"content\": prompt},\n",
        "    ]\n",
        ")\n",
        "\n",
        "print(response.choices[0].message.content)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MSFrew4Kx5kF",
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "Prompting without Engineering :"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "gather": {
          "logged": 1726010047262
        },
        "id": "BrGq_0dix5kG",
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "outputId": "3650fe76-16fa-4063-ab9d-4295ffd39e62"
      },
      "outputs": [],
      "source": [
        "prompt = \"Tell me about latest research of using foundational AI in Biometrics\"\n",
        "\n",
        "response = client.chat.completions.create(\n",
        "    model = \"gpt-4o-mini\",\n",
        "    messages=[\n",
        "        {\"role\": \"system\", \"content\": \"You are a helpful assistant\"},\n",
        "        {\"role\": \"user\", \"content\": prompt},\n",
        "    ]\n",
        ")\n",
        "\n",
        "print(response.choices[0].message.content)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5D1uaRct6s--"
      },
      "source": [
        "# Prompting Frameworks"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KFQQz6ww6cpZ"
      },
      "source": [
        "# CoT\n",
        "\n",
        "Chain-of-Thought esentially involved adding _\"Let's think step by step\"_ to the prompt"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FwPFOfyb6d9v",
        "outputId": "6e64e6de-2f6d-4709-ba36-a028811ac443"
      },
      "outputs": [],
      "source": [
        "prompt = \"Tell me about latest research of using foundational AI in Biometrics. Let's think step by step\"\n",
        "\n",
        "response = client.chat.completions.create(\n",
        "    model = \"gpt-4o-mini\",\n",
        "    messages=[\n",
        "        {\"role\": \"system\", \"content\": \"You are a helpful assistant\"},\n",
        "        {\"role\": \"user\", \"content\": prompt},\n",
        "    ]\n",
        ")\n",
        "\n",
        "print(response.choices[0].message.content)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_t_mWcDNx5kG",
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "# The RISEN Framework\n",
        "\n",
        "[The RISEN Framework](https://beeazt.com/knowledge-base/prompt-frameworks/the-risen-framework/)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "gather": {
          "logged": 1726010951125
        },
        "id": "KVH_Zq8sx5kG",
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "outputId": "734808b8-1dfa-4d7b-8638-6e068c000441"
      },
      "outputs": [],
      "source": [
        "prompt = \"\"\" Act as a researcher and find and summarize the latest research in using foundational AI in Biometrics. Here are some steps to follow:\n",
        "\n",
        "Find what were the main challenges in Biometrics.\n",
        "What are some ways Biometrics could leverage the latest GenAI?\n",
        "Explain the advantages of using GenAI to solve some of the challenges\n",
        "What was explored and experimented by researchers? WHat worked, what didn't really work?\n",
        "Are there newer problems which were discovered during the research and application of GenAI or LLMs in Biormetrics?\n",
        "Were there previous challenges that are easily solvable by foundational LLMs?\n",
        "\n",
        "Give an engaging and brief summary, around 500-700 words, that excites researchers to leverage LLM's to solve challenges in Biometrics leveraging GenAI\n",
        "Focus on the most pressing challenges\"\"\"\n",
        "\n",
        "\n",
        "response = client.chat.completions.create(\n",
        "    model = \"gpt-4o-mini\",\n",
        "    messages=[\n",
        "        {\"role\": \"system\", \"content\": \"You are a helpful assistant\"},\n",
        "        {\"role\": \"user\", \"content\": prompt},\n",
        "    ]\n",
        ")\n",
        "\n",
        "print(response.choices[0].message.content)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "n2v2d7BJx5kG",
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "# The APE Framework\n",
        "\n",
        "[The APE Framework](https://beeazt.com/knowledge-base/prompt-frameworks/the-abe-framework/):\n",
        "- Action: Specify the exact task or action the AI is expected to perform, providing clear direction and focus.\n",
        "- Purpose: Explain the rationale behind the prompt, helping the AI contextualize the request and tailor its response accordingly.\n",
        "- Expectation: Describe the anticipated format, detail, or outcome of the AI's response, setting clear standards for what constitutes a successful answer."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "gather": {
          "logged": 1726012015933
        },
        "id": "6-nQchY5x5kG",
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "outputId": "f15e8fb3-1d6a-4cd5-fe46-9ccfd262c548"
      },
      "outputs": [],
      "source": [
        "prompt = \"\"\"Find and summarize the latest research in using foundational AI in Biometrics.\n",
        "\n",
        "Help me get upto speed on latest research in using foundational AI in Biometrics.\n",
        "\n",
        "Give an engaging and brief summary, around 500-700 words, that excites researchers to leverage LLM's to solve challenges in Biometrics leveraging GenAI\n",
        "Focus on the most pressing challenges\"\"\"\n",
        "\n",
        "\n",
        "response = client.chat.completions.create(\n",
        "    model = \"gpt-4o-mini\",\n",
        "    messages=[\n",
        "        {\"role\": \"system\", \"content\": \"You are a helpful assistant\"},\n",
        "        {\"role\": \"user\", \"content\": prompt},\n",
        "    ]\n",
        ")\n",
        "\n",
        "print(response.choices[0].message.content)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0Lvp122Ux5kG",
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "# The CLEAR Framework\n",
        "\n",
        "(Challenge-Limitation-Effect-Action-Result)\n",
        "This is used to identify challenges, explain limitations, suggest possible actions, and anticipate future outcomes.\n",
        "\n",
        "Formula: Identify a [challenge], acknowledge a [Limitation], predict the [Effect], propose an [Action], and envisage the [Result].\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "gather": {
          "logged": 1726012639387
        },
        "id": "z2jVsQvLx5kH",
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "outputId": "da11702a-78bc-4f9e-93a9-2c85e58bdce3"
      },
      "outputs": [],
      "source": [
        "prompt = \"\"\"Its hard to keep one well versed with the latest research in Biometrics\n",
        "especially around using LLMs due to AI evolving at such a high speed. As a result we\n",
        "are often struggling to find the most excitinf and latest research in using\n",
        "foundational AI in Biometrics.\n",
        "\n",
        "Can you solve the above problem and update me latest research in Biometrics\n",
        "leveraging GenAI by giving an engaging and brief summary, around 500-700 words, that excites researchers to leverage LLM's to solve challenges in Biometrics leveraging GenAI\n",
        "Focus on the most pressing challenges\"\"\"\n",
        "\n",
        "\n",
        "response = client.chat.completions.create(\n",
        "    model = \"gpt-4o-mini\",\n",
        "    messages=[\n",
        "        {\"role\": \"system\", \"content\": \"You are a helpful assistant\"},\n",
        "        {\"role\": \"user\", \"content\": prompt},\n",
        "    ]\n",
        ")\n",
        "\n",
        "print(response.choices[0].message.content)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Wn3ZyL7Qx5kH",
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "# The CO-STAR Framework\n",
        "Context, Outcome, Scale, Time, Actor, Resources are main components of CO-STAR Framework"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "djN_E4Ghx5kH",
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "outputId": "9293abf7-21b9-4a67-cc3a-b7ff3092186a"
      },
      "outputs": [],
      "source": [
        "prompt = \"\"\"I am attending ICJB conference and I need to present a tutorial at the conference. As a result,\n",
        "I need to get upto speed with the latest research in Biometrics especially around using LLMs/foundational AI in Biometrics.\n",
        "\n",
        "Can you update me about latest research in Biometrics leveraging GenAI by giving an engaging and brief summary,\n",
        " that excites researchers to leverage LLM's to solve challenges in Biometrics. I will be\n",
        "presenting the workshop but target audience is students, enginners, experts in Biometrics or computer science\n",
        "Focus on the most pressing challenges. Response should be bullet point list of around  500-700 words\"\"\"\n",
        "\n",
        "\n",
        "response = client.chat.completions.create(\n",
        "    model = \"gpt-4o-mini\",\n",
        "    messages=[\n",
        "        {\"role\": \"system\", \"content\": \"You are a helpful assistant\"},\n",
        "        {\"role\": \"user\", \"content\": prompt},\n",
        "    ]\n",
        ")\n",
        "\n",
        "print(response.choices[0].message.content)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KPjFdXZyM_nz"
      },
      "source": [
        "# AUTOMAT framework"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oXS2eyozM4Vc",
        "outputId": "51b1ed42-7073-4c23-98c6-7d40f9e8c8cf"
      },
      "outputs": [],
      "source": [
        "prompt = \"\"\"Act as a research scientist who is attending ICJB conference and needs to present\n",
        "a tutorial at the conference for a diverse audience of students, enginners, experts in Biometrics or computer science\n",
        "and those interested in the field.\n",
        "Summarize the latest research to get him/her upto speed with the latest research in Biometrics especially\n",
        "around using LLMs/foundational AI in Biometrics in an engaging way, keeping output in bullet point list of\n",
        "around  500-700 words. Focus on the most pressing challenges. Don't provide anything generic, give concrete challenges.\n",
        "If you don't have enough information, return - Lack of info\"\"\"\n",
        "\n",
        "\n",
        "response = client.chat.completions.create(\n",
        "    model = \"gpt-4o-mini\",\n",
        "    messages=[\n",
        "        {\"role\": \"system\", \"content\": \"You are a helpful assistant\"},\n",
        "        {\"role\": \"user\", \"content\": prompt},\n",
        "    ]\n",
        ")\n",
        "\n",
        "print(response.choices[0].message.content)\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [
        "KFQQz6ww6cpZ"
      ],
      "provenance": []
    },
    "kernel_info": {
      "name": "python38-azureml"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.13"
    },
    "microsoft": {
      "host": {
        "AzureML": {
          "notebookHasBeenCompleted": true
        }
      },
      "ms_spell_check": {
        "ms_spell_check_language": "en"
      }
    },
    "nteract": {
      "version": "nteract-front-end@1.0.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
